# üìä Post-Deployment Monitoring Guide

## Overview

After deploying the CI/CD pipeline, continuous monitoring ensures system health, performance optimization, and early issue detection. This guide covers all aspects of monitoring your ML pipeline.

---

## üéØ Monitoring Objectives

```
‚úÖ Track pipeline execution health
‚úÖ Monitor model performance metrics
‚úÖ Detect anomalies and degradation
‚úÖ Ensure security compliance
‚úÖ Optimize resource usage
‚úÖ Enable quick debugging
‚úÖ Maintain audit trails
```

---

## üìà Key Metrics to Monitor

### **1. Pipeline Health Metrics**

#### Success Rate
```bash
# What to track
- CI Pipeline success rate (target: >95%)
- ML Tests success rate (target: >95%)
- CD Pipeline success rate (target: >95%)
- Overall pipeline success rate (target: >95%)

# How to check
gh run list --limit 50 | grep -c "COMPLETED"
# Divide by total runs
```

#### Execution Time
```bash
# What to track
- CI execution time (target: 3-5 min)
- ML Tests execution time (target: 5-8 min)
- CD execution time (target: 10-25 min)
- Daily training time (target: 15-30 min)

# How to check
gh run view <run-id> --json duration
```

#### Job-Level Metrics
```bash
# What to track
- Each job's pass/fail status
- Job execution time
- Job resource usage
- Failed step identification

# How to check
gh run view <run-id> --json jobs
```

---

### **2. Model Performance Metrics**

#### Classification Metrics
```
Accuracy (target: ‚â•65%)
  ‚îî‚îÄ Percentage of correct predictions
  
F1 Score (target: ‚â•0.65)
  ‚îî‚îÄ Balance between precision & recall
  
Precision (target: ‚â•0.65)
  ‚îî‚îÄ Correctness of positive predictions
  
Recall (target: ‚â•0.60)
  ‚îî‚îÄ Coverage of positive class
```

#### Regression Metrics
```
RMSE (Root Mean Squared Error)
  ‚îî‚îÄ Lower is better (target: <1.5)
  
R¬≤ Score (Coefficient of Determination)
  ‚îî‚îÄ Higher is better (target: >0.1)
```

#### Performance Tracking
```bash
# Location of metrics
models/manifest.json          # Latest model versions
models/v*_metadata.json       # Individual model metrics
performance_history.jsonl     # Daily performance history
```

#### Monitoring Script
```python
import json
import os
from collections import deque

def track_performance():
    """Extract and track model metrics"""
    models_dir = 'models/'
    
    # Get latest metadata
    json_files = sorted([f for f in os.listdir(models_dir) 
                        if 'metadata' in f and f.endswith('.json')])
    
    if json_files:
        latest = os.path.join(models_dir, json_files[-1])
        with open(latest) as f:
            metrics = json.load(f)
        
        print("Latest Model Metrics:")
        print(f"  Accuracy: {metrics.get('accuracy', 'N/A')}")
        print(f"  F1 Score: {metrics.get('f1_score', 'N/A')}")
        print(f"  RMSE: {metrics.get('rmse', 'N/A')}")
        
        # Check thresholds
        if metrics.get('accuracy', 0) < 0.65:
            print("‚ö†Ô∏è  WARNING: Accuracy below threshold!")
        
        return metrics
```

---

### **3. Data Quality Metrics**

#### Data Freshness
```bash
# What to track
- Last data update timestamp
- Data age (should be < 24 hours)
- API fetch success rate

# How to check
ls -lt data/raw/bitcoin_timeseries.csv | head -1
stat data/raw/bitcoin_timeseries.csv  # Check modification time
```

#### Data Completeness
```bash
# What to track
- Number of records (target: 365)
- Missing values (target: 0)
- Null values (target: 0)
- Data type consistency

# Validation script
python -c "
import pandas as pd
df = pd.read_csv('data/raw/bitcoin_timeseries.csv')
print(f'Records: {len(df)}')
print(f'Missing: {df.isnull().sum().sum()}')
print(f'Columns: {df.shape[1]}')
"
```

#### Data Consistency
```bash
# Check for duplicates
python -c "
import pandas as pd
df = pd.read_csv('data/raw/bitcoin_timeseries.csv')
dupes = df.duplicated().sum()
print(f'Duplicates: {dupes}')
"

# Check date range
python -c "
import pandas as pd
df = pd.read_csv('data/raw/bitcoin_timeseries.csv')
df['date'] = pd.to_datetime(df['date'])
print(f'Date range: {df[\"date\"].min()} to {df[\"date\"].max()}')
"
```

---

### **4. Resource Usage Metrics**

#### GitHub Actions Usage
```bash
# What to track
- Minutes used this month
- Limit: 2,000 min/month (private repos)
- Projected usage based on current runs

# How to check via CLI
gh api user/repos -H "Accept: application/vnd.github+json" | \
  jq '.[] | select(.name=="your-repo") | .owner'

# Or via GitHub UI
Settings ‚Üí Billing and plans ‚Üí Actions
```

#### Artifact Storage
```bash
# What to track
- Artifact size (target: minimize)
- Storage usage trend
- Retention policy compliance

# Monitor by checking
Actions tab ‚Üí Artifacts ‚Üí Size
```

#### Build Time Trends
```bash
# Track optimization over time
gh run list --limit 100 | \
  awk '{print $NF}' | \
  sort | \
  uniq -c | \
  tail -20
```

---

## üîç Monitoring Dashboard Setup

### **Option 1: GitHub Native Dashboard**

**Location**: `https://github.com/YOUR_USERNAME/YOUR_REPO/actions`

**What to Monitor:**
```
‚úÖ All Workflows tab
   ‚îî‚îÄ View all workflow runs
   
‚úÖ Individual Workflow tabs
   ‚îú‚îÄ CI pipeline runs
   ‚îú‚îÄ ML Tests runs
   ‚îú‚îÄ CD pipeline runs
   ‚îî‚îÄ Scheduled training runs
   
‚úÖ Workflow file status
   ‚îî‚îÄ Any syntax errors
   
‚úÖ Branch status checks
   ‚îî‚îÄ Pass/fail requirements
```

**Key Indicators:**
```
üü¢ Green checkmark   = All passed
üü° Yellow dot        = Running
üî¥ Red X             = Failed
‚ö´ Gray dot          = Skipped/Cancelled
```

---

### **Option 2: Local Monitoring Script**

Create `monitor_pipeline.py`:

```python
#!/usr/bin/env python3
"""
CI/CD Pipeline Monitoring Script
Provides real-time status of pipeline health
"""

import json
import os
import subprocess
from datetime import datetime, timedelta
from collections import defaultdict

def get_recent_runs(limit=20):
    """Get recent workflow runs using GitHub CLI"""
    try:
        result = subprocess.run(
            ['gh', 'run', 'list', '--limit', str(limit), '--json',
             'name,status,conclusion,createdAt,databaseId'],
            capture_output=True, text=True
        )
        if result.returncode == 0:
            return json.loads(result.stdout)
    except Exception as e:
        print(f"Error fetching runs: {e}")
    return []

def calculate_success_rate(runs):
    """Calculate pipeline success rate"""
    if not runs:
        return 0.0
    
    successful = sum(1 for r in runs if r.get('conclusion') == 'success')
    return (successful / len(runs)) * 100

def get_model_metrics():
    """Get latest model performance metrics"""
    models_dir = 'models/'
    metrics = {}
    
    if os.path.exists(models_dir):
        # Read latest metadata
        json_files = sorted([
            f for f in os.listdir(models_dir)
            if 'metadata' in f and f.endswith('.json')
        ])
        
        if json_files:
            latest = os.path.join(models_dir, json_files[-1])
            with open(latest) as f:
                metrics = json.load(f)
    
    return metrics

def check_data_freshness():
    """Check if data is recent"""
    data_file = 'data/raw/bitcoin_timeseries.csv'
    
    if os.path.exists(data_file):
        mod_time = os.path.getmtime(data_file)
        mod_datetime = datetime.fromtimestamp(mod_time)
        age_hours = (datetime.now() - mod_datetime).total_seconds() / 3600
        
        return {
            'last_updated': mod_datetime.isoformat(),
            'age_hours': age_hours,
            'fresh': age_hours < 24
        }
    
    return {'fresh': False, 'error': 'Data file not found'}

def print_status_report():
    """Print comprehensive status report"""
    print("\n" + "="*70)
    print(f"  CI/CD PIPELINE MONITORING REPORT")
    print(f"  Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70 + "\n")
    
    # Pipeline Health
    print("üìä PIPELINE HEALTH")
    print("-" * 70)
    runs = get_recent_runs(50)
    
    if runs:
        success_rate = calculate_success_rate(runs)
        print(f"  Success Rate (last 50 runs): {success_rate:.1f}%")
        print(f"  Status Target: >95%")
        
        if success_rate >= 95:
            print(f"  ‚úÖ HEALTHY")
        elif success_rate >= 85:
            print(f"  ‚ö†Ô∏è  WARNING")
        else:
            print(f"  ‚ùå CRITICAL")
        
        # Recent failures
        failures = [r for r in runs if r.get('conclusion') != 'success']
        if failures:
            print(f"\n  Recent Failures:")
            for f in failures[:3]:
                print(f"    ‚Ä¢ {f.get('name')} - {f.get('conclusion')}")
    
    # Model Performance
    print("\nüìà MODEL PERFORMANCE")
    print("-" * 70)
    metrics = get_model_metrics()
    
    if metrics:
        print(f"  Accuracy: {metrics.get('accuracy', 'N/A')}")
        print(f"  Target: ‚â•65%")
        
        accuracy = metrics.get('accuracy', 0)
        if accuracy >= 0.65:
            print(f"  ‚úÖ PASSING")
        else:
            print(f"  ‚ö†Ô∏è  BELOW THRESHOLD")
        
        print(f"\n  F1 Score: {metrics.get('f1_score', 'N/A')}")
        print(f"  RMSE: {metrics.get('rmse', 'N/A')}")
        print(f"  Training Samples: {metrics.get('training_samples', 'N/A')}")
        print(f"  Test Samples: {metrics.get('test_samples', 'N/A')}")
    
    # Data Freshness
    print("\nüóÇÔ∏è  DATA QUALITY")
    print("-" * 70)
    data_info = check_data_freshness()
    
    if data_info.get('fresh'):
        print(f"  ‚úÖ DATA FRESH")
    else:
        print(f"  ‚ö†Ô∏è  DATA STALE")
    
    print(f"  Last Updated: {data_info.get('last_updated', 'Unknown')}")
    print(f"  Age: {data_info.get('age_hours', 'Unknown')} hours")
    
    # Summary
    print("\n" + "="*70)
    print("  SUMMARY")
    print("="*70)
    
    status = "‚úÖ HEALTHY" if success_rate >= 95 else "‚ö†Ô∏è  NEEDS ATTENTION"
    print(f"  Overall Status: {status}")
    print(f"  Next Steps: Check GitHub Actions for details")
    print("="*70 + "\n")

if __name__ == '__main__':
    print_status_report()
```

**Usage:**
```bash
python monitor_pipeline.py

# Run continuously
while true; do python monitor_pipeline.py; sleep 300; done
```

---

## üö® Alert Conditions

### **Critical Alerts** (Immediate Action Required)

```
üî¥ CRITICAL - Pipeline Failure
   ‚îî‚îÄ Multiple consecutive failed runs
   ‚îî‚îÄ Action: Check logs, investigate immediately

üî¥ CRITICAL - Model Accuracy Degradation >10%
   ‚îî‚îÄ Accuracy drops from 70% to 60%
   ‚îî‚îÄ Action: Review training data, retrain

üî¥ CRITICAL - Security Vulnerabilities Found
   ‚îî‚îÄ Trivy scan detects high-severity issues
   ‚îî‚îÄ Action: Review and patch immediately

üî¥ CRITICAL - Data Missing or Corrupted
   ‚îî‚îÄ Data validation fails
   ‚îî‚îÄ Action: Check data source, restore backup
```

### **Warning Alerts** (Review & Monitor)

```
üü° WARNING - Success Rate < 90%
   ‚îî‚îÄ Action: Review failed runs, optimize

üü° WARNING - Model Accuracy < 65%
   ‚îî‚îÄ Action: Review model, consider retraining

üü° WARNING - Data Age > 24 hours
   ‚îî‚îÄ Action: Check API, verify schedule

üü° WARNING - Build Time > 30 minutes
   ‚îî‚îÄ Action: Optimize workflows, cache layers

üü° WARNING - Artifact Storage > 80% quota
   ‚îî‚îÄ Action: Clean old artifacts, increase retention
```

### **Info Alerts** (For Tracking)

```
‚ÑπÔ∏è  INFO - Successful pipeline run
   ‚îî‚îÄ Record metrics, update performance history

‚ÑπÔ∏è  INFO - Scheduled daily training complete
   ‚îî‚îÄ Log results, check metrics

‚ÑπÔ∏è  INFO - Weekly performance summary
   ‚îî‚îÄ Review trends, plan optimizations
```

---

## üìã Daily Monitoring Checklist

### **Morning Check (5 minutes)**
```
‚òê Check GitHub Actions dashboard
‚òê Verify all scheduled runs completed
‚òê Review model accuracy metrics
‚òê Check for any red/failed indicators
‚òê Note any issues to investigate
```

### **Daily Routine (10 minutes)**
```
‚òê Run monitoring script
‚òê Check pipeline success rate
‚òê Verify data freshness (< 24 hrs old)
‚òê Review recent model metrics
‚òê Check artifact storage usage
‚òê Monitor build time trends
```

### **Weekly Review (30 minutes)**
```
‚òê Generate performance report
‚òê Analyze trends (accuracy, time, success)
‚òê Review failed runs and root causes
‚òê Check security alerts
‚òê Update documentation if needed
‚òê Plan optimizations
```

### **Monthly Deep Dive (1 hour)**
```
‚òê Complete system audit
‚òê Review all metrics trends
‚òê Analyze resource usage
‚òê Check for performance improvements
‚òê Update baselines and targets
‚òê Plan next optimizations
```

---

## üìä Performance Tracking

### **Create Performance Dashboard**

```python
import pandas as pd
import json
from datetime import datetime, timedelta

def create_performance_report():
    """Generate performance report from history"""
    
    # Read performance history
    history = []
    if os.path.exists('performance_history.jsonl'):
        with open('performance_history.jsonl') as f:
            for line in f:
                history.append(json.loads(line))
    
    if not history:
        return None
    
    df = pd.DataFrame(history)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Last 7 days
    week_ago = datetime.utcnow() - timedelta(days=7)
    recent = df[df['timestamp'] > week_ago]
    
    report = {
        'period': 'Last 7 Days',
        'runs': len(recent),
        'avg_accuracy': recent['accuracy'].mean(),
        'max_accuracy': recent['accuracy'].max(),
        'min_accuracy': recent['accuracy'].min(),
        'avg_f1': recent['f1_score'].mean(),
        'trend': 'improving' if recent['accuracy'].iloc[-1] > recent['accuracy'].iloc[0] else 'declining'
    }
    
    print(f"Performance Report - {report['period']}")
    print(f"  Runs: {report['runs']}")
    print(f"  Avg Accuracy: {report['avg_accuracy']:.4f}")
    print(f"  Max Accuracy: {report['max_accuracy']:.4f}")
    print(f"  Min Accuracy: {report['min_accuracy']:.4f}")
    print(f"  Trend: {report['trend']}")
    
    return report
```

---

## üîî Notification Setup

### **Discord Notifications** (Already Configured)

```bash
# Set webhook
$env:DISCORD_WEBHOOK_URL = "your-webhook-url"

# Test notification
python -c "
import requests
import os

webhook = os.getenv('DISCORD_WEBHOOK_URL')
if webhook:
    requests.post(webhook, json={
        'content': '‚úÖ Pipeline monitoring active!'
    })
"
```

### **Slack Integration** (Optional)

```bash
# Set webhook
$env:SLACK_WEBHOOK_URL = "your-slack-webhook"

# Send notification
python -c "
import requests
import os

slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
if slack_webhook:
    requests.post(slack_webhook, json={
        'text': '‚úÖ Pipeline monitoring active!',
        'attachments': [{
            'color': 'good',
            'fields': [
                {'title': 'Status', 'value': 'Running', 'short': True}
            ]
        }]
    })
"
```

### **Email Alerts** (Optional)

```bash
# Configure in GitHub
Settings ‚Üí Notifications ‚Üí Email
  ‚úì Receive email notifications for workflow runs
```

---

## üìà Metrics Collection

### **Automated Metrics Logging**

The scheduled workflow automatically logs:
- Daily training timestamp
- Model accuracy
- F1 score
- RMSE
- R¬≤ score
- Training/test sample counts

Location: `performance_history.jsonl`

### **Manual Metrics Export**

```bash
# Export metrics to CSV
python -c "
import json
import pandas as pd

history = []
with open('performance_history.jsonl') as f:
    for line in f:
        history.append(json.loads(line))

df = pd.DataFrame(history)
df.to_csv('performance_metrics.csv', index=False)
print(f'Exported {len(df)} records')
"
```

---

## üîß Troubleshooting Monitoring Issues

### **"GitHub CLI not installed"**
```bash
# Install GitHub CLI
# Windows with Scoop:
scoop install gh

# Verify
gh --version
gh auth login  # Authenticate if needed
```

### **"Can't access artifact data"**
```bash
# Check permissions
gh auth status

# Verify token has repo access
gh auth refresh --scopes repo
```

### **"Performance history not found"**
```bash
# Create empty history file to start
touch performance_history.jsonl

# Or initialize from existing metrics
python -c "
import json
import os

# Read latest model metadata
models_dir = 'models/'
json_files = sorted([f for f in os.listdir(models_dir) 
                    if 'metadata' in f])
if json_files:
    with open(os.path.join(models_dir, json_files[-1])) as f:
        metrics = json.load(f)
    # Append to history
    with open('performance_history.jsonl', 'a') as f:
        f.write(json.dumps(metrics) + '\n')
"
```

---

## üìä Dashboard Tools (Optional)

### **Grafana** (Advanced)
```
1. Install Grafana
2. Connect to GitHub API data source
3. Create dashboards for:
   - Pipeline success rate
   - Model accuracy trends
   - Build time trends
   - Resource usage
```

### **Prometheus** (Advanced)
```
1. Export metrics in Prometheus format
2. Scrape GitHub Actions API
3. Set up alerting rules
4. Visualize in Grafana
```

### **Google Sheets** (Simple)
```
1. Create spreadsheet
2. Manually update daily:
   - Run date
   - Success rate
   - Model accuracy
   - Any issues
3. Create charts for trends
```

---

## üìã Monitoring Schedule

```
REAL-TIME (continuous)
  ‚îú‚îÄ GitHub Actions dashboard
  ‚îî‚îÄ Critical alerts

HOURLY (automated)
  ‚îú‚îÄ Log metrics to history
  ‚îî‚îÄ Check for failures

DAILY (manual + automated)
  ‚îú‚îÄ Morning status check
  ‚îú‚îÄ Scheduled training execution
  ‚îú‚îÄ Performance tracking
  ‚îî‚îÄ Alert review

WEEKLY (manual)
  ‚îú‚îÄ Comprehensive review
  ‚îú‚îÄ Trend analysis
  ‚îú‚îÄ Performance report
  ‚îî‚îÄ Optimization planning

MONTHLY (manual)
  ‚îú‚îÄ Full system audit
  ‚îú‚îÄ Baseline updates
  ‚îú‚îÄ Documentation update
  ‚îî‚îÄ Next quarter planning
```

---

## üéØ Success Criteria

| Metric | Target | Status |
|--------|--------|--------|
| CI Success Rate | >95% | Monitor |
| ML Tests Success Rate | >95% | Monitor |
| CD Success Rate | >95% | Monitor |
| Model Accuracy | ‚â•65% | Monitor |
| Build Time | <30 min | Optimize |
| Data Freshness | <24 hrs | Track |
| Artifact Storage | <80% quota | Manage |
| Security Alerts | 0 critical | Monitor |

---

## üìû Quick Commands Reference

```bash
# Check status
gh run list

# View specific run
gh run view <id>

# View logs
gh run view <id> --log

# Get workflows
gh workflow list

# Trigger workflow
gh workflow run scheduled-training.yml

# Monitor performance
python monitor_pipeline.py

# Export metrics
python -c "
import pandas as pd
import json
history = [json.loads(line) for line in open('performance_history.jsonl')]
pd.DataFrame(history).to_csv('metrics.csv')
"
```

---

## üöÄ Post-Deployment Monitoring Checklist

### **Week 1: Initial Setup**
- [ ] Access GitHub Actions dashboard
- [ ] Install GitHub CLI
- [ ] Run first monitoring script
- [ ] Set up Discord/Slack notifications
- [ ] Review first week of metrics
- [ ] Document baseline performance

### **Month 1: Stabilization**
- [ ] Daily monitoring routine established
- [ ] All alerts configured
- [ ] Performance baselines recorded
- [ ] Weekly reports generated
- [ ] No critical issues remaining
- [ ] Team trained on monitoring

### **Ongoing: Maintenance**
- [ ] Daily health checks
- [ ] Weekly performance reviews
- [ ] Monthly deep dives
- [ ] Quarterly optimizations
- [ ] Annual planning updates

---

**Status**: ‚úÖ **MONITORING READY**

All monitoring components configured and documented!
